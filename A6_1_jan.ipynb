{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "A6_1_jan.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "toc_visible": true,
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jansoe/BootstrapPrediction/blob/master/A6_1_jan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RX02S6Aq8OxF"
   },
   "source": [
    "# 6. Wortvektoren - Teil 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sao3gLY7kXug"
   },
   "source": [
    "## 6.1 Word2Vec selbst trainieren\n",
    "\n",
    "Nachdem wir uns im letzten Teil mit bereits trainierten Wortvektoren beschäftigt haben, zeigen wir Ihnen hier, wie Sie ein Word2Vec-Modell anhand eines Textdatensatzes trainieren können.\n",
    "\n",
    "Das Verfahren ist wie gehabt:\n",
    "- Erstellen Sie eine Kopie dieses Notebooks in ihrem Google Drive (vorgeschlagene Umbenennung: \"A6_1 - Vorname, Nachname\")\n",
    "- Editieren Sie die Text- und Codezellen.\n",
    "- Schicken Sie uns einen Freigabelink zum Kommentieren Ihres Notebooks an"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OzpoF4R3Ch2I"
   },
   "source": [
    "### Vorbereitungen\n",
    "\n",
    "Zunächst das Übliche: Verwendete Bibliotheken importieren und Warnungen ausschalten."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "c6sSUrDuDXkX"
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import gensim\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Wh2V3ZvpPLb"
   },
   "source": [
    "Als Datensatz verwenden wir die Wikipedia-Artikel, die wir bereits in Abschnitt 5 heruntergeladen und vorbereitet haben. Entsprechend müssen wir unser Google Drive einbinden."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "D7cSjXHNAwx5",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 128
    },
    "outputId": "b32f7c07-b252-4db3-d1ae-d334be2f45e9"
   },
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5mFwxZYOqDz_"
   },
   "source": [
    "Ersetzen Sie in der folgenden Code-Zelle die Pfadangabe mit dem enstprechenden Verzeichnis Ihres Google Drives."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WlWbMS4NQPSy",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "outputId": "5b3a2825-68e7-4ebc-e9f7-57edb00fdf6c"
   },
   "source": [
    "path = '/content/drive/My Drive/KI-Schule/2 - Textdaten/wiki'\n",
    "!ls \"{path}\""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "83EdFeNnkyyl"
   },
   "source": [
    "Jetzt sollten die Wikipedia-Artikel aus A5_1 geladen werden können.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "aBvNO6EJpr_v"
   },
   "source": [
    "import json\n",
    "corpus = json.load(open(path + '/article_dict.json'))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KmvGoryhqhS-"
   },
   "source": [
    "Jetzt müssen wir die Rohdaten noch in eine für das Training des Modells geeignete Form bringen. Hierfür definieren wir uns wieder den bereits bekannten Tokenizer ..."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zafHSXbnA8hb"
   },
   "source": [
    "def tokenizer(text):\n",
    "    tokens = list(gensim.utils.simple_tokenize(text))\n",
    "    return tokens"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oJdh8AXxrly8"
   },
   "source": [
    "... und definieren uns eine Funktion zum Rausfiltern von Stopwords auf Basis einer zuvor heruntergelandenen Liste."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JQp8oQK5C2vh",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "outputId": "2586ba49-a364-4b42-af0d-6e3e0dc36348"
   },
   "source": [
    "nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words('german')\n",
    "def is_not_stopword(word):\n",
    "    return word not in stopwords"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_lZyDeEXqE-2"
   },
   "source": [
    "Im nächsten Schritt können wir unsere Funktionen `tokenizer()` und `is_not_stopword()` anwenden, um uns eine vorverarbeitete Dokumentensammlung zu erstellen. Dieser Schritt kann einige Zeit in Anspruch nehmen."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qSh2NOCJFCwR",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "outputId": "6f6a00c1-dfb3-4c94-a293-0003339ba98b"
   },
   "source": [
    "n_articles =  15000 # 15000 Artikel ermöglichen recht hohe Qualität; noch mehr dauert entsprechend länger\n",
    "\n",
    "docs = []\n",
    "for n_article, text in enumerate(list(corpus.values())[:n_articles]):\n",
    "    tokens = tokenizer(text.lower())\n",
    "    tokens = list(filter(is_not_stopword, tokens))\n",
    "    if len(tokens) > 5:\n",
    "        docs.append(tokens)\n",
    "    if n_article % 100 == 0:\n",
    "        print('\\r', n_articles - n_article, 'articles still need processing', end='')\n",
    "print('fertig!')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CTX4YGpZlKIr"
   },
   "source": [
    "### Ein Word2Vec-Modell trainieren\n",
    "\n",
    "Anhand dieser Zusammenstellung von tokenisierten Dokumenten können wir nun ein Word2Vec-Modell trainieren. Auch dieser Schritt benötigt einige Minuten - leider ohne jegliche Form von Fortschrittsanzeige."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KFYREljOShWR"
   },
   "source": [
    "my_model = gensim.models.Word2Vec(\n",
    "    sentences = docs,\n",
    "    window=15,\n",
    "    size=150,\n",
    "    min_count=50\n",
    ")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BHj8EqXP6CHD"
   },
   "source": [
    "Sie können anschließend das Modell in ihrem Google Drive abspeichern, damit Sie das Training nur einmal absolvieren müssen."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Go5Nu5JCwY6B"
   },
   "source": [
    "my_model.save(path + \"/my_word2vec.model\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HiDdhoC56q-w"
   },
   "source": [
    "Bei Bedarf können Sie das abgespeicherte Modell zu einem späteren Zeitpunkt wieder laden.\n",
    "\n",
    "---\n",
    "**Anmerkungen**\n",
    "\n",
    "Auch wenn wir im Rahmen dieses Notebooks hiervon nicht Gebrauch machen, könnte das Modell über die Methode train() immer wieder mit weiteren Texten trainiert werden - allerdings muss es sich dabei um Texte des gleichen Korpus handeln, da sich das Vokabular (und damit die Länge der Input-Vektoren) bei dieser Art von Modellen im Nachhinein nicht mehr erweitert werden kann:\n",
    "\n",
    "`my_model.train(new_corpus, total_examples=1, epochs=1)`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "amj-wLNW61_s"
   },
   "source": [
    "my_model = gensim.models.Word2Vec.load(path + \"/my_word2vec.model\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FmuFI7fv9XCF"
   },
   "source": [
    "Wir können das Modell kurz testen, indem wir uns z.B. die fünf Wörter ausgeben lassen, die dem Begriff \"Intelligenz\" am ähnlichsten sind (auf Basis der Kosinusähnlichkeit der zugehörigen Wordvektoren)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5sZnVS7wBsGa",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "outputId": "ee3ed080-f3bc-4de6-b4f3-daf5c33397c2"
   },
   "source": [
    "my_model.wv.most_similar(positive=['intelligenz'], topn=5)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kMONQATY-7Ch"
   },
   "source": [
    "### 6.1.0 Grenzen des Modells\n",
    "\n",
    "Können Sie ein Wort finden, dessen fünf ähnlichste Begriffe Sie nicht überzeugen?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SG5svONAByPC"
   },
   "source": [
    "### Visualisierung ähnlicher Wortvektoren"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cZfiUlkj8cBW"
   },
   "source": [
    "top_similarity = my_model.wv.most_similar(positive=['intelligenz'], topn=1000)\n",
    "selected_words = [word for word, similartiy in top_similarity]\n",
    "vectors = [my_model.wv.word_vec(word, use_norm=True) for word in selected_words]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qFI4xI2XGfc8",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "outputId": "22e6c495-22f8-40ea-82a6-0f72ed137e8a"
   },
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(\n",
    "    n_components=2,\n",
    "    perplexity = 30\n",
    ")\n",
    "vectors_2D = tsne.fit_transform(vectors)\n",
    "\n",
    "to_plot = pd.DataFrame(vectors_2D, columns=['x', 'y'])\n",
    "to_plot['labels'] = selected_words\n",
    "\n",
    "px.scatter(to_plot, x='x', y='y', hover_name='labels')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5jotEfZl2gID"
   },
   "source": [
    "![insitubytes](https://drive.google.com/uc?id=1EAJK7AI9tcZRo3VvYq7vEKGxk7vmK2Ff)"
   ]
  }
 ]
}